{
    "doc": {
        "base": "base optimizer configuration for unet and text encoder",
        "text_encoder_overrides": "text encoder config overrides",
        "-----------------": "-----------------",
        "optimizer": "adamw, adamw8bit, lion",
        "optimizer_desc": "'adamw' in standard 32bit, 'adamw8bit' is bitsandbytes, 'lion' is lucidrains",
        "lr": "learning rate, if null will use CLI or main JSON config value",
        "lr_scheduler": "'constant' or 'cosine'",
        "lr_warmup_steps": "number of steps to warmup LR to target LR, if null will use CLI or default a value based on max epochs",
        "lr_decay_steps": "number of steps to decay LR to zero for cosine, if null will use CLI or default a value based on max epochs",
        "betas": "exponential decay rates for the moment estimates",
        "epsilon": "value added to denominator for numerical stability, unused for lion",
        "weight_decay": "weight decay (L2 penalty)",
        "------------------": "-----------------",
        "unfreeze_last_n_layers": "if not null, freeze all parameters in the text encoder except for the last n layers and the final layer norm"
    },
    "base": {
        "optimizer": "adamw8bit",
        "lr": 4.0e-06,
        "lr_decay_steps": 30750,
        "lr_scheduler": "constant",
        "lr_warmup_steps": 0,
        "betas": [0.9, 0.999],
        "epsilon": 1e-8,
        "weight_decay": 0.010
    },
    "text_encoder_overrides": {
        "optimizer": null,
        "lr": 1e-08,
        "lr_scheduler": "constant",
        "lr_decay_steps": 30750,
        "lr_warmup_steps": 0,
        "betas": null,
        "epsilon": null,
        "weight_decay": null
    },
    "text_encoder_freezing": {
        "unfreeze_last_n_layers": null
    },
    "apply_grad_scaler_step_tweaks": true
}
